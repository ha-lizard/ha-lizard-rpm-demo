#!/bin/bash
####################################
# Configuration File for ha-lizard #
####################################

######################################################################################
# This file rarely needs to be modified. HA-Lizard configuration parameters are
# centrally stored and shared with all pool members via the XAPI database. A CLI
# tool, 'ha-cfg' can be run on ANY pool host and configuration changes will propogate
# to all members of a pool making it unnecessary to manage configurations on
# each host within a pool.
#
# Changing this file will override the shared pool configuration parameters and should
# be used only in cases where a configuration setting change should be made to a
# specific host in the pool.
#######################################################################################


###############################################
# Set whether to enable posting version number
# installed to the developer. Only the version
# number is passed to the developer by the 
# installer one time.
###############################################
POST_VERSION=1

#################################################
# Set the time to suppress duplicate email alerts
# in minutes. Prevents duplicate email alerts 
# from being sent until timer expires
#################################################
MAIL_SCREEN_TIME=60

#################################################
# Set the number of seconds to tolerate loss of
# the management link on the pool MASTER. On
# expiry of this time, ALL VMs running on the
# master will be forcefully shutdown
#################################################
#MGT_LINK_LOSS_TOLERANCE=5

#################################################
# Select the method used to select a host
# to start a VM on.
# 0 = intelligent mode. A host is selected 
#     based on its health which is tracked every few seconds.
# 1 = legacy mode. A slave failure will also 
#     trigger a loss of XenCenter connectivity
# Default value = 0 (recommended setting is 0)
#################################################
#HOST_SELECT_METHOD=0

#################################################
# Select whether to enable hourly disk health
# monitoring which triggers an alert on any
# non-normal SMART flags. Disk or RAID controller
# must support SMART for this feature to work
#################################################
#DISK_MONITOR=1

###############################################
# Set the maximum wait time for call to xe
# to respond. If xe does not respond within
# XE_TIMEOUT seconds, the xe PID will be killed
# This is done to prevent xe calls from hanging
# in the event of a Master host failure.
################################################
#XE_TIMEOUT=5

##############################################
# Set ha-lizard monitor timers
#
# MONITOR_MAX_STARTS: Threshold for when
# to assume running process is not responding.
# Sets how many failed starts to wait before
# killing any hung processes. Default = 20
#
# MONITOR_KILLALL: If MAX_MONITOR_STARTS
# threshhold is reached - set whether to kill
# all ha-lizard processes. Default = 1
# 1 = yes, 0 = no
#
# MONITOR_DELAY: Delay in seconds between
# re-spawning ha-lizard. This should be adjusted
# to the environment. Large pools require more
# time for each run of ha-lizard. Default = 45
#
# MONITOR_SCANRATE: ha-lizard will not re-spawn
# unless all current processes are completed.
# If there are active processes while attempting
# to start a new run, ha-lizard will wait the 
# number of seconds set here before retrying.
# Each successive fail will increment a counter
# that may trigger KILLALL. Default = 10
#################################################
#MONITOR_MAX_STARTS=20
#MONITOR_DELAY=45
#MONITOR_KILLALL=1
#MONITOR_SCANRATE=10

#######################################
# Enable / Disable ha-lizard for pool
# This is set within Xencenter management
# console. Custom field "ha-lizard-enabled"
# should be created for the pool.
# default custom field name can be 
# altered here.
# Field accepted values = true/false
######################################
#XC_FIELD_NAME='ha-lizard-enabled'

#######################################
# Set the operating mode for ha-lizard
# 1 = manage appliances
# 2 = manage virtual machines
#######################################
#OP_MODE=2

#######################################
# Set whether to individually enable
# HA on each VM within the pool
# (when OP_MODE = 2)
# 
# 0 = You must individually set
# ha-lizard-enabled to true/false for each
# VM within the pool
#
# 1 = ALL VMs have HA enabled regarless
# of setting in GUI/CLI
#
# Default value = 1 
#######################################
#GLOBAL_VM_HA=1

#######################################
# Enable Logging 1=yes, 0=no
# logs written to /var/log/messages
# All log messages labeled with "ha-lizard"
# View/Filter real time logging with:
# tail -f /var/log/messages | grep ha-lizard
#######################################
#ENABLE_LOGGING=1

######################################
# Enable alerts 1=yes 0=no
# sets pool-wide alerts which can be
# viewed or managed via management
# tools (such as xencenter) or CLI
# Alerts mirror email alerts
######################################
#ENABLE_ALERTS=1

######################################################################
# Specify UUID(s) of vapps that do not get automatically started
# by ha-lizard. Array is ":" delimited like this (UUID1:UUID2:UUID3)
# Leave blank if ALL vapps are managed by ha-lizard "DISABLED_VAPPS=()"
# Only applied when OP_MODE=1
######################################################################
#DISABLED_VAPPS=()

#####################################################################
# If a pool member  cannot reach a pool peer set the following:
# XAPI_COUNT is the number of retry attempts when a host failure is
# detected.
# XAPI_DELAY is the number of seconds to wait in between XAPI_COUNT
# attempts contact host
# Default XAPI_COUNT = 5 and DEFAULT XAPI_DELAY = 15 seconds
##################################################################### 
#XAPI_COUNT=5
#XAPI_DELAY=2

####################################################################
# If the Pool Master cannot be reached and all attempts to reach
# it have been exhausted, set whether autoselected slave will try to
# start appliances and/or VMs.
# (PROMOTE_SLAVE must also be set to 1 for this to work)
####################################################################
#SLAVE_HA=1

####################################################################
# If master cannot be reched - set whether slave should be promoted
# to pool master (this only affects a single slave: the 
# "autoselect" winner chosen by the former master to recover the pool)
####################################################################
#PROMOTE_SLAVE=1

###################################################################
# By default, only the pool master will check the status of all
# VMs managed by this script and attempt to start a VM that is not
# in the running state. Setting SLAVE_VM_STAT to 1 will cause
# any pool slaves to also check all VM statuses and attempt to 
# start any VM not in the running state. Default = 0
# In a large pool many hosts may attempt to start the same VM
# the first host to attempt will succeed, others will be safely declined
# Enabliing may create many unnecesary duplicate processes in the pool
###################################################################
#SLAVE_VM_STAT=0

##################################################################
# Email Alert Settings
##################################################################
#MAIL_ON=1
#MAIL_SUBJECT="SYSTEM ALERT - FROM HOST: $HOSTNAME" 
#MAIL_FROM="root@localhost"
#MAIL_TO='root@localhost'
#SMTP_SERVER="127.0.0.1"
#SMTP_PORT=25
#SMTP_USER=""
#SMTP_PASS=""


##################################################################
# Fencing Configuration
# Currently Supported FENCE_METHOD = ILO, XVM, POOLi, IRMC
# (XVM is intended for test environments with nested xen instances where pool dom0s are domus within a single xen host)
# (POOL does not fence failed hosts. It simply allows the forceful removal of a failed host from a pool and acts as a dummy fence method)
#
# FENCE_ENABLED: 		Enable/Disable Fencing 1=enabled 0=disabled 
# FENCE_FILE_LOC: 		Location to store and look for fencing scripts
# FENCE_HA_ONFAIL:		Select whether to attmept starting of failed hosts VMs on another host if fencing fails 1=enabled 0=disabled
# FENCE_METHOD:			HP ILO, XVM and POOL  supported. "FENCE_METHOD=ILO" - custom fencing scripts can be added and called here
# FENCE_PASSWD:			Password for fence device
# FENCE_ACTION:			Supported actions = start, stop, reset
# FENCE_REBOOT_LONE_HOST:	If this host cannot see any other pool members, choose whether to reboot before attempting to fence peers 1=enabled 0=disabled
# FENCE_IPADDRESS:		Only used for static fencing device - currently XVM host supported
# FENCE_HOST_FORGET:		Select whether to forget a fenced host (permanently remove from pool) 1=enabled 0=disabled
# FENCE_MIN_HOSTS:		Do not allow fencing when fewer than this number of hosts are remaining in the pool
# FENCE_QUORUM_REQUIRED:	Select whether pool remaining hosts should have quorum before allowing fencing. 1=enabled 0=disabled
# FENCE_USE_IP_HEURISTICS:	Select whether to check list of IP Addresses to create another quorum vote. 1=enabled 0=disabled
# FENCE_HEURISTICS_IPS:		":" seperated list if IP addresses to check if FENCE_USE_IP_HEURISTICS is enabled.
###################################################################
#FENCE_ENABLED=1
#FENCE_FILE_LOC=/etc/ha-lizard/fence
#FENCE_HA_ONFAIL=1
#FENCE_METHOD=POOL
#FENCE_PASSWD=
#FENCE_ACTION=stop
#FENCE_REBOOT_LONE_HOST=0
#FENCE_IPADDRESS=
#FENCE_HOST_FORGET=1
#FENCE_MIN_HOSTS=3
#FENCE_QUORUM_REQUIRED=1
#FENCE_USE_IP_HEURISTICS=1
#FENCE_HEURISTICS_IPS=()

# Below enables / disables logging per function 1=enable 0=disable
LOG_check_logger_processes=1
LOG_log_experimental=1
LOG_log=1
LOG_xe_raise_alert=1
LOG_email=1
LOG_xe_wrapper=1
LOG_xe_variable_wrapper=1
LOG_watch_proc=1
LOG_get_pool_host_list=1
LOG_get_pool_ip_list=1
LOG_master_ip=1
LOG_ha_disabled=1
LOG_check_xapi=1
LOG_get_app_vms=1
LOG_vm_state=1
LOG_vm_state_check=1
LOG_vm_mon=1
LOG_validate_vm_safe_to_start_here=1
LOG_validate_vm_home_pool=1
LOG_vm_start=1
LOG_promote_slave=1
LOG_get_vms_on_host=1
LOG_get_vms_on_host_local=1
LOG_check_vm_managed=1
LOG_write_pool_state=1
LOG_check_slave_status=1
LOG_fence_host=1
LOG_autoselect_slave=1
LOG_check_ha_enabled=1
LOG_check_quorum=1
LOG_check_logging_enabled=1
LOG_check_email_enabled=1
LOG_check_xs_ha=1
LOG_disable_ha_lizard=1
LOG_update_global_conf_params=1
LOG_check_master_mgt_link_state=1
LOG_stop_vms_on_host=1
LOG_stop_vms_on_host_slow=1
LOG_service_execute=1
LOG_make_box=1
LOG_write_status_report=1
LOG_validate_vm_ha_state=1
LOG_reset_vm_vdi=1
LOG_validate_this_host_vm_states_1=1
LOG_validate_this_host_vm_states=1
LOG_get_alert_level=0

# Below enables / disables email alerts per function 1=enable 0=disable
MAIL_check_logger_processes=1
MAIL_log_experimental=1
MAIL_log=1
MAIL_xe_raise_alert=1
MAIL_email=1
MAIL_xe_wrapper=1
MAIL_xe_variable_wrapper=1
MAIL_watch_proc=1
MAIL_get_pool_host_list=1
MAIL_get_pool_ip_list=1
MAIL_master_ip=1
MAIL_ha_disabled=1
MAIL_check_xapi=1
MAIL_get_app_vms=1
MAIL_vm_state=1
MAIL_vm_state_check=1
MAIL_vm_mon=1
MAIL_validate_vm_safe_to_start_here=1
MAIL_validate_vm_home_pool=1
MAIL_vm_start=1
MAIL_promote_slave=1
MAIL_get_vms_on_host=1
MAIL_get_vms_on_host_local=1
MAIL_check_vm_managed=1
MAIL_write_pool_state=1
MAIL_check_slave_status=1
MAIL_fence_host=1
MAIL_autoselect_slave=1
MAIL_check_ha_enabled=1
MAIL_check_quorum=1
MAIL_check_logging_enabled=1
MAIL_check_email_enabled=1
MAIL_check_xs_ha=1
MAIL_disable_ha_lizard=1
MAIL_update_global_conf_params=1
MAIL_check_master_mgt_link_state=1
MAIL_stop_vms_on_host=1
MAIL_stop_vms_on_host_slow=1
MAIL_service_execute=1
MAIL_make_box=1
MAIL_write_status_report=1
MAIL_validate_vm_ha_state=1
MAIL_reset_vm_vdi=1
MAIL_validate_this_host_vm_states_1=1
MAIL_validate_this_host_vm_states=1
MAIL_get_alert_level=0

# Below sets alert priority level 0=no alert 1-5=severity of alert with 1=highest priority
PRIORITY_check_logger_processes=0
PRIORITY_log_experimental=0
PRIORITY_log=0
PRIORITY_xe_raise_alert=0
PRIORITY_email=0
PRIORITY_xe_wrapper=1
PRIORITY_xe_variable_wrapper=1
PRIORITY_watch_proc=1
PRIORITY_get_pool_host_list=3
PRIORITY_get_pool_ip_list=3
PRIORITY_master_ip=3
PRIORITY_ha_disabled=1
PRIORITY_check_xapi=1
PRIORITY_get_app_vms=3
PRIORITY_vm_state=3
PRIORITY_vm_state_check=3
PRIORITY_vm_mon=1
PRIORITY_validate_vm_safe_to_start_here=3
PRIORITY_validate_vm_home_pool=3
PRIORITY_vm_start=1
PRIORITY_promote_slave=1
PRIORITY_get_vms_on_host=3
PRIORITY_get_vms_on_host_local=3
PRIORITY_check_vm_managed=3
PRIORITY_write_pool_state=3
PRIORITY_check_slave_status=1
PRIORITY_fence_host=1
PRIORITY_autoselect_slave=3
PRIORITY_check_ha_enabled=3
PRIORITY_check_quorum=1
PRIORITY_check_logging_enabled=0
PRIORITY_check_email_enabled=0
PRIORITY_check_xs_ha=1
PRIORITY_disable_ha_lizard=1
PRIORITY_update_global_conf_params=3
PRIORITY_check_master_mgt_link_state=1
PRIORITY_stop_vms_on_host=1
PRIORITY_stop_vms_on_host_slow=1
PRIORITY_service_execute=0
PRIORITY_make_box=0
PRIORITY_write_status_report=3
PRIORITY_validate_vm_ha_state=3
PRIORITY_reset_vm_vdi=1
PRIORITY_validate_this_host_vm_states_1=1
PRIORITY_validate_this_host_vm_states=1
PRIORITY_get_alert_level=0
#END CONFIG FILE


